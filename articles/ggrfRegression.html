<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ggRandomForests: Exploring randomForestSRC Regression • ggRandomForests</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="ggRandomForests: Exploring randomForestSRC Regression">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ggRandomForests</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/ggrfRegression.html">ggRandomForests: Exploring randomForestSRC Regression</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/ehrlinger/ggRandomForests/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>ggRandomForests: Exploring randomForestSRC
Regression</h1>
                        <h4 data-toc-skip class="author">John
Ehrlinger</h4>
                  <a class="author_email" href="mailto:#"></a><a href="mailto:john.ehrlinger@gmail.com" class="email">john.ehrlinger@gmail.com</a>
      
                  
            <h4 data-toc-skip class="date">March 04, 2025</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/ehrlinger/ggRandomForests/blob/master/vignettes/ggrfRegression.Rmd" class="external-link"><code>vignettes/ggrfRegression.Rmd</code></a></small>
      <div class="hidden name"><code>ggrfRegression.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="abtract">Abtract<a class="anchor" aria-label="anchor" href="#abtract"></a>
</h2>
<p>Random Forests <span class="citation">(Leo Breiman 2001)</span> (RF)
are a non-parametric statistical method requiring no distributional
assumptions on covariate relation to the response. RF are a robust,
nonlinear technique that optimizes predictive accuracy by fitting an
ensemble of trees to stabilize model estimates. The package <span class="citation">(Ishwaran and Kogalur 2014)</span> is a unified
treatment of Breiman’s random forests for survival, regression and
classification problems.</p>
<p>Predictive accuracy make RF an attractive alternative to parametric
models, though complexity and interpretability of the forest hinder
wider application of the method. We introduce the package, tools for
visually understand random forest models grown in <code>R</code> <span class="citation">( Core Team 2014)</span> with the package. The package
is structured to extract intermediate data objects from objects and
generates figures using the <span class="citation">(Wickham 2009)</span>
graphics package</p>
<p>This document is structured as a tutorial for building random forests
for regression with the package and using the package for investigating
how the forest is constructed. We investigate the Boston Housing data
<span class="citation">Belsley, Kuh, and Welsch (1980)</span>. We
demonstrate random forest variable selection using Variable Importance
(VIMP) <span class="citation">(Leo Breiman 2001)</span> and Minimal
Depth <span class="citation">(Ishwaran et al. 2010)</span>, a property
derived from the construction of each tree within the forest. We will
also demonstrate the use of variable dependence plots <span class="citation">(Friedman 2000)</span> to aid interpretation RF
results. We then examine variable interactions between covariates using
a minimal depth interactions, and conditional variable dependence plots.
The goal of the exercise is to demonstrate the strength of using Random
Forest methods for both prediction and information retrieval in
regression settings.</p>
</div>
<div class="section level2">
<h2 id="about-this-document">About this document<a class="anchor" aria-label="anchor" href="#about-this-document"></a>
</h2>
<p>This document is a package vignette for the package for “Visually
Exploring Random Forests” (<a href="https://cran.r-project.org/package=ggRandomForests" class="external-link uri">https://cran.r-project.org/package=ggRandomForests</a>). The
package is designed for use with the package (<a href="https://cran.r-project.org/package=randomForestSRC" class="external-link uri">https://cran.r-project.org/package=randomForestSRC</a>)
<span class="citation">(Ishwaran and Kogalur 2014)</span> for growing
random forests for survival (time to event response), regression
(continuous response) and classification (categorical response) settings
and uses the package (<a href="https://cran.r-project.org/package=ggplot2" class="external-link uri">https://cran.r-project.org/package=ggplot2</a>) <span class="citation">(Wickham 2009)</span> for plotting diagnostic and
variable association results. is structured to extract data objects from
objects and provides functions for printing and plotting these
objects.</p>
<p>The vignette is a tutorial for using the package with the package for
building and post-processing random forests for regression settings. In
this tutorial, we explore a random forest for regression model
constructed for the Boston housing data set <span class="citation">Belsley, Kuh, and Welsch (1980)</span>, available in
the <code>MASS</code> package <span class="citation">(Venables and
Ripley 2002)</span>. We grow a random forest and demonstrate how can be
used when determining how the response depends on predictive variables
within the model. The tutorial demonstrates the design and usage of many
of functions and features and also how to modify and customize the
resulting <code>ggplot</code> graphic objects along the way.</p>
<p>The vignette is written in using the <code>knitr</code> package (<a href="https://cran.r-project.org/package=knitr" class="external-link uri">https://cran.r-project.org/package=knitr</a>]<span class="citation">Xie (2013)</span>, which facilitates weaving
<code>R</code> <span class="citation">( Core Team 2014)</span> code,
results and figures into document text. Throughout this document,
<code>R</code> code will be displayed in <em>code blocks</em> as shown
below. This code block loads the <code>R</code> packages required to run
the source code listed in code blocks throughout the remainder of this
document.</p>
<pre class="text"><code>&gt; ################## Load packages ##################
&gt; library("ggplot2")         # Graphics engine
&gt; library("RColorBrewer")    # Nice color palettes
&gt; library("plotly")          # for 3d surfaces. 
&gt; library("dplyr")           # Better data manipulations
&gt; library("tidyr")           # gather variables into long format
&gt; library("parallel")        # mclapply for multicore processing
&gt; 
&gt; # Analysis packages.
&gt; library("randomForestSRC") # random forests for survival, regression and 
&gt; # classification
&gt; library("ggRandomForests") # ggplot2 random forest figures (This!)
&gt; 
&gt; ################ Default Settings ##################
&gt; theme_set(theme_bw())     # A ggplot2 theme with white background
&gt; 
&gt; ## Set open circle for censored, and x for events 
&gt; event_marks &lt;- c(1, 4)
&gt; event_labels &lt;- c(FALSE, TRUE)
&gt; 
&gt; ## We want red for death events, so reorder this set.
&gt; str_col &lt;- brewer.pal(3, "Set1")[c(2, 1, 3)]</code></pre>
<p>This vignette is available within the package on the Comprehensive R
Archive Network (CRAN) (<a href="https://www.r-project.org" class="external-link uri">https://www.r-project.org</a>) <span class="citation">( Core
Team 2014)</span>. Once the package has been installed, the vignette can
be viewed directly from within <code>R</code> with the following
command:</p>
<pre class="text"><code>&gt; vignette("randomForestSRC-Regression", package = "ggRFVignette")</code></pre>
<p>A development version of the package is also available on GitHub (<a href="https://github.com" class="external-link uri">https://github.com</a>). We invite
comments, feature requests and bug reports for this package at (<a href="https://github.com/ehrlinger/ggRandomForests" class="external-link uri">https://github.com/ehrlinger/ggRandomForests</a>).</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Random Forests <span class="citation">(Leo Breiman 2001)</span> (RF)
are a fully non-parametric statistical method which requires no
distributional or functional assumptions on covariate relation to the
response. RF is a robust, nonlinear technique that optimizes predictive
accuracy by fitting an ensemble of trees to stabilize model estimates.
Random Survival Forests (RSF) <span class="citation">Ishwaran et al.
(2008)</span> are an extension of Breiman’s RF techniques to survival
settings, allowing efficient non-parametric analysis of time to event
data. The package <span class="citation">(Ishwaran and Kogalur
2014)</span> is a unified treatment of Breiman’s random forests for
survival (time to event response), regression (continuous response) and
classification (categorical response) problems.</p>
<p>Predictive accuracy make RF an attractive alternative to parametric
models, though complexity and interpretability of the forest hinder
wider application of the method. We introduce the package for visually
exploring random forest models. The package is structured to extract
intermediate data objects from objects and generate figures using the
graphics package <span class="citation">(Wickham 2009)</span>.</p>
<p>Many of the figures created by the package are also available
directly from within the package. However offers the following
advantages:</p>
<ul>
<li><p>Separation of data and figures: contains functions that operate
on either the <code><a href="https://www.randomforestsrc.org//reference/rfsrc.html" class="external-link">randomForestSRC::rfsrc</a></code> forest object
directly, or on the output from post processing functions
(i.e. <code>plot.variable</code>, <code>var.select</code>,
<code>find.interaction</code>) to generate intermediate data objects.
Functions are provide to further process these objects and plot results
using the graphics package. Alternatively, users can use these data
objects for their own custom plotting or analysis operations.</p></li>
<li><p>Each data object/figure is a single, self contained object. This
allows simple modification and manipulation of the data or objects to
meet users specific needs and requirements.</p></li>
<li><p>The use of for plotting. We chose to use the package for our
figures to allow users flexibility in modifying the figures to their
liking. Each plot function returns either a single <code>ggplot</code>
object, or a <code>list</code> of <code>ggplot</code> objects, allowing
users to use additional functions or themes to modify and customize the
figures to their liking.</p></li>
</ul>
<p>This document is formatted as a tutorial for using the package for
building and post-processing random forest models with the package for
investigating how the forest is constructed. In this tutorial, we use
the Boston Housing Data (), available in the <code>MASS</code> package
<span class="citation">(Venables and Ripley 2002)</span>, to build a
random forest for regression () and demonstrate the tools in the package
for examining the forest construction.</p>
<p>Random forests are not parsimonious, but use all variables available
in the construction of a response predictor. We demonstrate a random
forest variable selection () process using the Variable Importance ()
measure (VIMP) <span class="citation">(Leo Breiman 2001)</span> as well
as Minimal Depth () <span class="citation">(Ishwaran et al.
2010)</span>, a property derived from the construction of each tree
within the forest, to assess the impact of variables on forest
prediction.</p>
<p>Once we have an idea of which variables we are want to investigate
further, we will use variable dependence plots <span class="citation">(Friedman 2000)</span> to understand how a variable is
related to the response. Marginal variable dependence () plots give us
an idea of the overall trend of a variable/response relation, while
partial dependence () plots show us a risk adjusted relation. These
figures may show strongly non-linear variable/response relations that
are not easily obtained through a parametric approach. We are also
interested in examining variable interactions () within the forest
model. Using a minimal depth approach, we can quantify how closely
variables are related within the forest, and generate marginal
dependence coplots() and partial dependence coplots () (risk adjusted)
conditioning plots (coplots) <span class="citation">Cleveland
(1993)</span> to examine these interactions graphically.</p>
</div>
<div class="section level2">
<h2 id="data-boston-housing-values">Data: Boston Housing Values<a class="anchor" aria-label="anchor" href="#data-boston-housing-values"></a>
</h2>
<p>The Boston Housing data is a standard benchmark data set for
regression models. It contains data for 506 census tracts of Boston from
the 1970 census <span class="citation">Belsley, Kuh, and Welsch
(1980)</span>. The data is available in multiple <code>R</code>
packages, but to keep the installation dependencies for the package
down, we will use the data contained in the <code>MASS</code> package
(<a href="https://cran.r-project.org/package=MASS" class="external-link uri">https://cran.r-project.org/package=MASS</a>) <span class="citation">(Venables and Ripley 2002)</span>, available with the
base install of <code>R</code>. The following code block loads the data
into the environment. We include a table of the Boston data set variable
names, types and descriptions for reference when we interpret the model
results.</p>
<pre class="text"><code>&gt; # Load the Boston Housing data
&gt; data(Boston, package = "MASS")
&gt; 
&gt; # Set modes correctly. For binary variables: transform to logical
&gt; Boston$chas &lt;- as.logical(Boston$chas)</code></pre>
<table class="table">
<caption>
<code>Boston</code> housing data dictionary.</caption>
<colgroup>
<col width="10%">
<col width="79%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
<th align="left">type</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">crim</td>
<td align="left">Crime rate by town.</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">zn</td>
<td align="left">Proportion of residential land zoned for lots over
25,000 sq.ft.</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">indus</td>
<td align="left">Proportion of non-retail business acres per town.</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">chas</td>
<td align="left">Charles River (tract bounds river).</td>
<td align="left">logical</td>
</tr>
<tr class="odd">
<td align="left">nox</td>
<td align="left">Nitrogen oxides concentration (10 ppm).</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">rm</td>
<td align="left">Number of rooms per dwelling.</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="left">Proportion of units built prior to 1940.</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">dis</td>
<td align="left">Distances to Boston employment center.</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">rad</td>
<td align="left">Accessibility to highways.</td>
<td align="left">integer</td>
</tr>
<tr class="even">
<td align="left">tax</td>
<td align="left">Property tax rate per $10,000.</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">ptratio</td>
<td align="left">Pupil teacher ratio by town.</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">black</td>
<td align="left">Proportion of blacks by town.</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">lstat</td>
<td align="left">Lower status of the population (percent).</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">medv</td>
<td align="left">Median value of homes ($1000s).</td>
<td align="left">numeric</td>
</tr>
</tbody>
</table>
<p>The main objective of the Boston Housing data is to investigate
variables associated with predicting the median value of homes
(continuous <code>medv</code> response) within 506 suburban areas of
Boston.</p>
<div class="section level3">
<h3 id="exploratory-data-analysis">Exploratory Data Analysis<a class="anchor" aria-label="anchor" href="#exploratory-data-analysis"></a>
</h3>
<p>It is good practice to view your data before beginning an analysis,
what<span class="citation">(Tukey 1977)</span> refers to as Exploratory
Data Analysis (EDA). To facilitate this, we use figures with the
<code><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">ggplot2::facet_wrap</a></code> command to create two sets of panel
plots, one for categorical variables with boxplots at each level, and
one of scatter plots for continuous variables. Each variable is plotted
along a selected continuous variable on the X-axis. These figures help
to find outliers, missing values and other data anomalies in each
variable before getting deep into the analysis. We have also created a
separate <code>shiny</code> app (<a href="https://shiny.rstudio.com" class="external-link uri">https://shiny.rstudio.com</a>) <span class="citation">(Chang
et al. 2015)</span>, available at (<a href="https://ehrlinger.shinyapps.io/xportEDA" class="external-link uri">https://ehrlinger.shinyapps.io/xportEDA</a>), for creating
similar figures with an arbitrary data set, to make the EDA process
easier for users.</p>
<p>The Boston housing data consists almost entirely of continuous
variables, with the exception of the “Charles river” logical variable. A
simple EDA visualization to use for this data is a single panel plot of
the continuous variables, with observation points colored by the logical
variable. Missing values in our continuous variable plots are indicated
by the rug marks along the x-axis, of which there are none in this data.
We used the Boston housing response variable, the median value of homes
(<code>medv</code>), for X variable.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/eda-1.png" alt="EDA variable plots. Points indicate variable value against the median home value variable. Points are colored according to the chas variable." width="672"><p class="caption">
EDA variable plots. Points indicate variable value against the median
home value variable. Points are colored according to the chas variable.
</p>
</div>
<p>This figure is loosely related to a pairs scatter plot <span class="citation">(Becker, Chambers, and Wilks 1988)</span>, but in this
case we only examine the relation between the response variable against
the remainder. Plotting the data against the response also gives us a
“sanity check” when viewing our model results. It’s pretty obvious from
this figure that we should find a strong relation between median home
values and the <code>lstat</code> and <code>rm</code> variables.</p>
</div>
</div>
<div class="section level2">
<h2 id="random-forest---regression">Random Forest - Regression<a class="anchor" aria-label="anchor" href="#random-forest---regression"></a>
</h2>
<p>A Random Forest is grown by <em>bagging</em> <span class="citation">(L. Breiman 1996a)</span> a collection of
<em>classification and regression trees</em> (CART) <span class="citation">(L. Breiman et al. 1984)</span>. The method uses a set
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
bootstrap <span class="citation">(Efron and Tibshirani 1994)</span>
samples, growing an independent tree model on each sub-sample of the
population. Each tree is grown by recursively partitioning the
population based on optimization of a <em>split rule</em> over the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-dimensional
covariate space. At each split, a subset of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>≤</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">m \le p</annotation></semantics></math>
candidate variables are tested for the split rule optimization, dividing
each node into two daughter nodes. Each daughter node is then split
again until the process reaches the <em>stopping criteria</em> of either
<em>node purity</em> or <em>node member size</em>, which defines the set
of <em>terminal (unsplit) nodes</em> for the tree. In regression trees,
the split rule is based on minimizing the mean squared error, whereas in
classification problems, the Gini index is used <span class="citation">(Friedman 2000)</span>.</p>
<p>Random Forests sort each training set observation into one unique
terminal node per tree. Tree estimates for each observation are
constructed at each terminal node, among the terminal node members. The
Random Forest estimate for each observation is then calculated by
aggregating, averaging (regression) or votes (classification), the
terminal node results across the collection of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
trees.</p>
<p>For this tutorial, we grow the random forest for regression using the
<code>rfsrc</code> command to predict the median home value
(<code>medv</code> variable) using the remaining 13 independent
predictor variables. For this example we will use the default set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">B=1000</annotation></semantics></math>
trees (<code>ntree</code> argument),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">m=5</annotation></semantics></math>
candidate variables (<code>mtry</code>) for each split with a stopping
criteria of at most <code>nodesize=5</code> observations within each
terminal node.</p>
<p>Because growing random forests are computationally expensive, and the
package is targeted at the visualization of random forest objects, we
will use cached copies of the objects throughout this document. We
include the cached objects as data sets in the package. The actual
<code>rfsrc</code> calls are included in comments within code
blocks.</p>
<pre class="text"><code>&gt; # Load the data, from the call:
&gt; rfsrc_boston &lt;- rfsrc(medv ~ ., data = Boston, 
+                       importance = TRUE, err.block = 5)
&gt; 
&gt; # print the forest summary
&gt; rfsrc_boston</code></pre>
<p>The <code><a href="https://www.randomforestsrc.org//reference/print.rfsrc.html" class="external-link">randomForestSRC::print.rfsrc</a></code> summary details the
parameters used for the <code>rfsrc</code> call described above, and
returns variance and generalization error estimate from the forest
training set. The forest is built from 506 observations and 13
independent variables. It was constructed for the continuous
<code>medv</code> variable using <code>ntree=1000</code> regression
(<code>regr</code>) trees, randomly selecting 5 candidate variables at
each node split, and terminating nodes with no fewer than 5
observations.</p>
<div class="section level3">
<h3 id="generalization-error-estimates">Generalization error estimates<a class="anchor" aria-label="anchor" href="#generalization-error-estimates"></a>
</h3>
<p>One advantage of Random Forests is a built in generalization error
estimate. Each bootstrap sample selects approximately 63.2% of the
population on average. The remaining 36.8% of observations, the
Out-of-Bag (OOB) <span class="citation">(L. Breiman 1996b)</span>
sample, can be used as a hold out test set for each of the trees in the
forest. An OOB prediction error estimate can be calculated for each
observation by predicting the response over the set of trees which were
NOT trained with that particular observation. The Out-of-Bag prediction
error estimates have been shown to be nearly identical to n–fold cross
validation estimates <span class="citation">(Hastie, Tibshirani, and
Friedman 2009)</span>. This feature of Random Forests allows us to
obtain both model fit and validation in one pass of the algorithm.</p>
<p>The <code>gg_error</code> function operates on the
<code><a href="https://www.randomforestsrc.org//reference/rfsrc.html" class="external-link">randomForestSRC::rfsrc</a></code> object to extract the error
estimates as the forest is grown. The code block demonstrates part the
design philosophy, to create separate data objects and provide functions
to operate on the data objects. The following code block first creates a
<code>gg_error</code> object, then uses the <code>plot.gg_error</code>
function to create a <code>ggplot</code> object for display.</p>
<pre class="text"><code>&gt; # Plot the OOB errors against the growth of the forest.
&gt; gg_e &lt;- gg_error(rfsrc_boston)
&gt; gg_e &lt;- gg_e %&gt;% filter(!is.na(error))
&gt; class(gg_e) &lt;- c("gg_error", class(gg_e))
&gt; plot(gg_e)</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/error-1.png" alt="Random forest generalization error. OOB error convergence along the number of trees in the forest." width="700"><p class="caption">
Random forest generalization error. OOB error convergence along the
number of trees in the forest.
</p>
</div>
<p>This figure demonstrates that it does not take a large number of
trees to stabilize the forest prediction error estimate. However, to
ensure that each variable has enough of a chance to be included in the
forest prediction process, we do want to create a rather large random
forest of trees.</p>
</div>
<div class="section level3">
<h3 id="random-forest-prediction">Random Forest Prediction<a class="anchor" aria-label="anchor" href="#random-forest-prediction"></a>
</h3>
<p>The <code>gg_rfsrc</code> function extracts the OOB prediction
estimates from the random forest. This code block executes the the data
extraction and plotting in one line, since we are not interested in
holding the prediction estimates for later reuse. Also note that we add
in the additional command (<code>coord_cartesian</code>) to modify the
plot object. Each of the plot commands return <code>ggplot</code>
objects, which we can also store for modification or reuse later in the
analysis.</p>
<pre class="text"><code>&gt; # Plot predicted median home values.
&gt; plot(gg_rfsrc(rfsrc_boston), alpha = .5) +
+   coord_cartesian(ylim = c(5, 49))</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/rfsrc-1.png" alt="OOB predicted median home values. Points are jittered to help visualize predictions for each observation. Boxplot indicates the distribution of the predicted values." width="700"><p class="caption">
OOB predicted median home values. Points are jittered to help visualize
predictions for each observation. Boxplot indicates the distribution of
the predicted values.
</p>
</div>
<p>The <code>gg_rfsrc</code> plot shows the predicted median home value,
one point for each observation in the training set. The points are
jittered around a single point on the x-axis, since we are only looking
at predicted values from the forest. These estimates are Out of Bag,
which are analogous to test set estimates. The boxplot is shown to give
an indication of the distribution of the prediction estimates. For this
analysis the figure is another model sanity check, as we are more
interested in exploring the <strong>why</strong> questions for these
predictions.</p>
</div>
</div>
<div class="section level2">
<h2 id="variable-selection">Variable Selection<a class="anchor" aria-label="anchor" href="#variable-selection"></a>
</h2>
<p>Random forests are not parsimonious, but use all variables available
in the construction of a response predictor. Also, unlike parametric
models, Random Forests do not require the explicit specification of the
functional form of covariates to the response. Therefore there is no
explicit p-value/significance test for variable selection with a random
forest model. Instead, RF ascertain which variables contribute to the
prediction through the split rule optimization, optimally choosing
variables which separate observations. We use two separate approaches to
explore the RF selection process, Variable Importance
(\autoref{variable-importance) and Minimal Depth
(\autoref{minimal-depth).</p>
<div class="section level3">
<h3 id="variable-importance">Variable Importance<a class="anchor" aria-label="anchor" href="#variable-importance"></a>
</h3>
<p><em>Variable importance</em> (VIMP) was originally defined for CART
using a measure involving surrogate variables (see Chapter 5 of <span class="citation">(L. Breiman et al. 1984)</span>). The most popular VIMP
method uses a prediction error approach involving “noising-up” each
variable in turn. VIMP for a variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>v</mi></msub><annotation encoding="application/x-tex">x_v</annotation></semantics></math>
is the difference between prediction error when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>v</mi></msub><annotation encoding="application/x-tex">x_v</annotation></semantics></math>
is noised up by randomly permuting its values, compared to prediction
error under the observed values <span class="citation">Ishwaran et al.
(2008)</span>.</p>
<p>Since VIMP is the difference between OOB prediction error before and
after permutation, a large VIMP value indicates that misspecification
detracts from the variable predictive accuracy in the forest. VIMP close
to zero indicates the variable contributes nothing to predictive
accuracy, and negative values indicate the predictive accuracy
<em>improves</em> when the variable is misspecified. In the later case,
we assume noise is more informative than the true variable. As such, we
ignore variables with negative and near zero values of VIMP, relying on
large positive values to indicate that the predictive power of the
forest is dependent on those variables.</p>
<p>The <code>gg_vimp</code> function extracts VIMP measures for each of
the variables used to grow the forest. The <code>plot.gg_vimp</code>
function shows the variables, in VIMP rank order, from the largest
(Lower Status) at the top, to smallest (Charles River) at the bottom.
VIMP measures are shown using bars to compare the scale of the error
increase under permutation.</p>
<pre class="text"><code>&gt; # Plot the VIMP rankings of independent variables.
&gt; plot(gg_vimp(rfsrc_boston), lbls = st_labs)</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/vimp-1.png" alt="Random forest VIMP plot. Bars are colored by sign of VIMP, longer blue bars indicate more important variables." width="672"><p class="caption">
Random forest VIMP plot. Bars are colored by sign of VIMP, longer blue
bars indicate more important variables.
</p>
</div>
<p>For our random forest, the top two variables (<code>lstat</code> and
<code>rm</code>) have the largest VIMP, with a sizable difference to the
remaining variables, which mostly have similar VIMP measure. This
indicates we should focus attention on these two variables, at least,
over the others.</p>
<p>In this example, all VIMP measures are positive, though some are
small. When there are both negative and positive VIMP values, the
<code>plot.gg_vimp</code> function will color VIMP by the sign of the
measure. We use the <code>lbls</code> argument to pass a named
<code>vector</code> of meaningful text descriptions to the
<code>plot.gg_vimp</code> function, replacing the often terse variable
names used by default.</p>
</div>
<div class="section level3">
<h3 id="minimal-depth">Minimal Depth<a class="anchor" aria-label="anchor" href="#minimal-depth"></a>
</h3>
<p>In VIMP, prognostic risk factors are determined by testing the forest
prediction under alternative data settings, ranking the most important
variables according to their impact on predictive ability of the forest.
An alternative method uses inspection of the forest construction to rank
variables. <em>Minimal depth</em> assumes that variables with high
impact on the prediction are those that most frequently split nodes
nearest to the trunks of the trees (i.e. at the root node) where they
partition large samples of the population.</p>
<p>Within a tree, node levels are numbered based on their relative
distance to the trunk of the tree (with the root at 0). Minimal depth
measures the important risk factors by averaging the depth of the first
split for each variable over all trees within the forest. Lower values
of this measure indicate variables important in splitting large groups
of patients.</p>
<p>The <em>maximal subtree</em> for a variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is the largest subtree whose root node splits on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.
All parent nodes of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>’s
maximal subtree have nodes that split on variables other than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.
The largest maximal subtree possible is at the root node. If a variable
does not split the root node, it can have more than one maximal subtree,
or a maximal subtree may also not exist if there are no splits on the
variable. The minimal depth of a variables is a surrogate measure of
predictiveness of the variable. The smaller the minimal depth, the more
impact the variable has sorting observations, and therefore on the
forest prediction.</p>
<p>The <code>gg_minimal_depth</code> function is analogous to the
<code>gg_vimp</code> function for minimal depth. Variables are ranked
from most important at the top (minimal depth measure), to least at the
bottom (maximal minimal depth). The vertical dashed line indicates the
minimal depth threshold where smaller minimal depth values indicate
higher importance and larger indicate lower importance.</p>
<p>The <code><a href="https://www.randomforestsrc.org//reference/var.select.rfsrc.html" class="external-link">randomForestSRC::var.select</a></code> call is again a
computationally intensive function, as it traverses the forest finding
the maximal subtree within each tree for each variable before averaging
the results we use in the <code>gg_minimal_depth</code> call. We again
use the cached object strategy here to save computational time. The
<code>var.select</code> call is included in the comment of this code
block.</p>
<pre class="text"><code>&gt; # Load the data, from the call:
&gt; varsel_boston &lt;- var.select(rfsrc_boston)
&gt; 
&gt; # Save the gg_minimal_depth object for later use.
&gt; gg_md &lt;- gg_minimal_depth(varsel_boston)
&gt; 
&gt; # plot the object
&gt; plot(gg_md, lbls = st_labs)</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/minimaldepth-1.png" alt="Minimal Depth variables in rank order, most important at the top. Vertical dashed line indicates the maximal minimal depth for important variables." width="672"><p class="caption">
Minimal Depth variables in rank order, most important at the top.
Vertical dashed line indicates the maximal minimal depth for important
variables.
</p>
</div>
<p>In general, the selection of variables according to VIMP is to rather
arbitrarily examine the values, looking for some point along the ranking
where there is a large difference in VIMP measures. The minimal depth
threshold method has a more quantitative approach to determine a
selection threshold. Given minimal depth is a quantitative property of
the forest construction, <span class="citation">(Ishwaran et al.
2010)</span> also construct an analytic threshold for evidence of
variable impact. A simple optimistic threshold rule uses the mean of the
minimal depth distribution, classifying variables with minimal depth
lower than this threshold as important in forest prediction. The minimal
depth plot for our model indicates there are ten variables which have a
higher impact (minimal depth below the mean value threshold) than the
remaining three.</p>
<p>Since the VIMP and Minimal Depth measures use different criteria, we
expect the variable ranking to be somewhat different. We use
<code>gg_minimal_vimp</code> function to compare rankings between
minimal depth and VIMP. In this call, we plot the stored
<code>gg_minimal_depth</code> object (<code>gg_md</code>), which would
be equivalent to calling
<code>plot.gg_minimal_vimp(varsel_boston)</code> or
<code>plot(gg_minimal_vimp(varsel_boston))</code>.</p>
<pre class="text"><code>&gt; # gg_minimal_depth objects contain information about
&gt; # both minimal depth and VIMP.
&gt; plot(gg_minimal_vimp(gg_md))</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/minimalvimp-1.png" alt="Comparing Minimal Depth and Vimp rankings. Points on the red dashed line are ranked equivalently, points below have higher VIMP, those above have higher minimal depth ranking. Variables are colored by the sign of the VIMP measure." width="700"><p class="caption">
Comparing Minimal Depth and Vimp rankings. Points on the red dashed line
are ranked equivalently, points below have higher VIMP, those above have
higher minimal depth ranking. Variables are colored by the sign of the
VIMP measure.
</p>
</div>
<p>The points along the red dashed line indicates where the measures are
in agreement. Points above the red dashed line are ranked higher by VIMP
than by minimal depth, indicating the variables are sensitive to
misspecification. Those below the line have a higher minimal depth
ranking, indicating they are better at dividing large portions of the
population. The further the points are from the line, the more the
discrepancy between measures. The construction of this figure is skewed
towards a minimal depth approach, by ranking variables along the y-axis,
though points are colored by the sign of VIMP.</p>
<p>In our example, both minimal depth and VIMP indicate the strong
relation of <code>lstat</code> and <code>rm</code> variables to the
forest prediction, which agrees with our expectation from the Data:
Boston Housing Values () done at the beginning of this document. We now
turn to investigating how these, and other variables, are related to the
predicted response.</p>
</div>
</div>
<div class="section level2">
<h2 id="responsevariable-dependence">Response/Variable Dependence<a class="anchor" aria-label="anchor" href="#responsevariable-dependence"></a>
</h2>
<p>As random forests are not a parsimonious methodology, we can use the
minimal depth and VIMP measures to reduce the number of variables we
need to examine to a manageable subset. We would like to know how the
forest response depends on some specific variables of interest. We often
choose to examine variables of interest based on the study question, or
other previous knowledge. In the absence of this, we will look at
variables that contribute most to the predictive accuracy of the
forest.</p>
<p>Although often characterized as a “black box” method, it is possible
to express a random forest in functional form. In the end the forest
predictor is some function, although complex, of the predictor variables
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>f</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>f</mi></mrow></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\hat{f}_{rf} = f(x).</annotation></semantics></math>
We use graphical methods to examine the forest predicted response
dependency on covariates. We again have two options, Variable Dependence
() plots are quick and easy to generate, and Partial Dependence
(\autoref{partial-dependence) plots are computationally intensive but
give us a risk adjusted look at the dependence.</p>
<div class="section level3">
<h3 id="variable-dependence">Variable Dependence<a class="anchor" aria-label="anchor" href="#variable-dependence"></a>
</h3>
<p>Variable dependence plots show the predicted response as a function
of a covariate of interest, where each observation is represented by a
point on the plot. Each predicted point is an individual observations,
dependent on the full combination of all other covariates, not only on
the covariate of interest. Interpretation of variable dependence plots
can only be in general terms, as point predictions are a function of all
covariates in that particular observation. However, variable dependence
is straight forward to calculate, only requiring the predicted response
for each observation.</p>
<p>We use the <code>gg_variable</code> function call to extract the
training set variables and the predicted OOB response from
<code><a href="https://www.randomforestsrc.org//reference/rfsrc.html" class="external-link">randomForestSRC::rfsrc</a></code> and
<code>randomForestSRC::predict</code> objects. In the following code
block, we will store the <code>gg_variable</code> data object for later
use, as all remaining variable dependence plots can be constructed from
this (<code>gg_v</code>) object. We will also use the minimal depth
selected variables (minimal depth lower than the threshold value) from
the previously stored <code>gg_minimal_depth</code> object
(<code>gg_md$topvars</code>) to filter the variables of interest.</p>
<p>The <code>plot.gg_variable</code> function call operates in the
<code>gg_variable</code> object. We pass it the list of variables of
interest (<code>xvar</code>) and request a single panel
(<code>panel=TRUE</code>) to display the figures. By default, the
<code>plot.gg_variable</code> function returns a list of
<code>ggplot</code> objects, one figure for each variable named in
<code>xvar</code> argument. The next three arguments are passed to
internal <code>ggplot</code> plotting routines. The <code>se</code> and
<code>span</code> arguments are used to modify the internal call to
<code><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html" class="external-link">ggplot2::geom_smooth</a></code> for fitting smooth lines to the data.
The <code>alpha</code> argument lightens the coloring points in the
<code><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">ggplot2::geom_point</a></code> call, making it easier to see point
over plotting. We also demonstrate modification of the plot labels using
the <code><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ggplot2::labs</a></code> function.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/variable-1.png" alt="Variable dependence plot. Individual case predictions are marked with points. Loess smooth curve indicates the trend as the variables increase with shaded 95\% confidence band." width="672"><p class="caption">
Variable dependence plot. Individual case predictions are marked with
points. Loess smooth curve indicates the trend as the variables increase
with shaded 95% confidence band.
</p>
</div>
<p>This figure looks very similar to the Data: Boston Housing Values
(\autoref{data-boston-housing-values) figure, although with transposed
axis as we plot the response variable on the y-axis. The closer the
panels match, the better the RF prediction. The panels are sorted to
match the order of variables in the <code>xvar</code> argument and
include a smooth loess line <span class="citation">Cleveland and Devlin
(1988)</span>, with 95% shaded confidence band, to indicates the trend
of the prediction dependence over the covariate values.</p>
<p>There is not a convenient method to panel scatter plots and boxplots
together, so we recommend creating panel plots for each variable type
separately. The Boston housing data does contain a single categorical
variable, the Charles river logical variable. Variable dependence plots
for categorical variables are constructed using boxplots to show the
distribution of the predictions within each category. Although the
Charles river variable has the lowest importance scores in both VIMP and
minimal depth measures, we include the variable dependence plot as an
example of categorical variable dependence.</p>
<pre class="text"><code>&gt; plot(gg_v, xvar = "chas", alpha = 0.4) +
+   labs(y = st_labs["medv"])
&gt; 
&gt; # , points=FALSE, se=FALSE, notch=TRUE</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/chas-1.png" alt="Variable dependence for Charles River logical variable." width="700"><p class="caption">
Variable dependence for Charles River logical variable.
</p>
</div>
<p>The figure shows that most housing tracts do not border the Charles
river (<code>chas=FALSE</code>), and comparing the distributions of the
predicted median housing values indicates no significant difference in
home values. This reinforces the findings in both VIMP and Minimal
depth, the Charles river variable has very little impact on the forest
prediction of median home values.</p>
</div>
<div class="section level3">
<h3 id="partial-dependence">Partial Dependence<a class="anchor" aria-label="anchor" href="#partial-dependence"></a>
</h3>
<p>Partial variable dependence plots are a risk adjusted alternative to
variable dependence. Partial plots are generated by integrating out the
effects of all variables beside the covariate of interest. Partial
dependence data are constructed by selecting points evenly spaced along
the distribution of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
variable of interest. For each value
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math>),
we calculate the average RF prediction over all other covariates in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mover><mi>f</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex"> \tilde{f}(x) = \frac{1}{n} \sum_{i = 1}^n \hat{f}(x, x_{i, o}), </annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>f</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{f}</annotation></semantics></math>
is the predicted response from the random forest and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">x_{i, o}</annotation></semantics></math>
is the value for all other covariates other than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math>
for the observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math><span class="citation">(Friedman 2000)</span>. Essentially, we average a
set of predictions for each observation in the training set at the value
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X=x</annotation></semantics></math>.
We repeating the process for a sequence of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X=x</annotation></semantics></math>
values to generate the estimated points to create a partial dependence
plot.</p>
<p>Partial plots are another computationally intensive analysis,
especially when there are a large number of observations. We again turn
to our data caching strategy here. The default parameters for the
<code><a href="https://www.randomforestsrc.org//reference/plot.variable.rfsrc.html" class="external-link">randomForestSRC::plot.variable</a></code> function generate partial
dependence estimates at <code>npts=25</code> points (the default value)
along the variable of interest. For each point of interest, the
<code>plot.variable</code> function averages <code>n</code> response
predictions. This is repeated for each of the variables of interest and
the results are returned for later analysis.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/partial-1.png" alt="Partial dependence panels. Risk adjusted variable dependence for variables in minimal depth rank order." width="672"><p class="caption">
Partial dependence panels. Risk adjusted variable dependence for
variables in minimal depth rank order.
</p>
</div>
<p>We again order the panels by minimal depth ranking. We see again how
the <code>lstat</code> and <code>rm</code> variables are strongly
related to the median value response, making the partial dependence of
the remaining variables look flat. We also see strong nonlinearity of
these two variables. The <code>lstat</code> variable looks rather
quadratic, while the <code>rm</code> shape is more complex.</p>
<p>We could stop here, indicating that the RF analysis has found these
ten variables to be important in predicting the median home values. That
increasing <code>lstat</code> (percentage population of lower status)
values are associated with decreasing median home values
(<code>medv</code>) and increasing <code>rm &gt; 6</code> (number of
rooms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">&gt; 6</annotation></semantics></math>)
are associated with increasing median home values. However, we may also
be interested in investigating how these variables work together to help
improve the random forest prediction of median home values.</p>
</div>
</div>
<div class="section level2">
<h2 id="variable-interactions">Variable Interactions<a class="anchor" aria-label="anchor" href="#variable-interactions"></a>
</h2>
<p>Using the different variable dependence measures, it is also possible
to calculate measures of pairwise interactions among variables. Recall
that minimal depth measure is defined by averaging the tree depth of
variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
relative to the root node. To detect interactions, this calculation can
be modified to measure the minimal depth of a variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
with respect to the maximal subtree for variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math><span class="citation">Ishwaran et al. (2011)</span>.</p>
<p>The <code><a href="https://www.randomforestsrc.org//reference/find.interaction.rfsrc.html" class="external-link">randomForestSRC::find.interaction</a></code> function traverses
the forest, calculating all pairwise minimal depth interactions, and
returns a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math>
matrix of interaction measures. For each row, the diagonal terms are are
related to the minimal depth relative to the root node, though
normalized to minimize scaling issues. Each off diagonal minimal depth
term is relative to the diagonal term on that row. Small values indicate
that an off diagonal term typically splits close to the diagonal term,
indicating an forest split proximity of the two variables.</p>
<p>The <code>gg_interaction</code> function wraps the
<code>find.interaction</code> matrix for use with the provided plot and
print functions. The <code>xvar</code> argument indicates which
variables we’re interested in looking at. We again use the cache
strategy, and collect the figures together using the
<code>panel=TRUE</code> option.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/interactions-1.png" alt="Minimal depth variable interactions. Reference variables are marked with red cross in each panel. Higher values indicate lower interactivity with reference variable." width="672"><p class="caption">
Minimal depth variable interactions. Reference variables are marked with
red cross in each panel. Higher values indicate lower interactivity with
reference variable.
</p>
</div>
<p> plots the interactions for the target variable (shown in the red
cross) with interaction scores for all remaining variables. We expect
the covariate with lowest minimal depth (<code>lstat</code>) to be
associated with almost all other variables, as it typically splits close
to the root node, so viewed alone it may not be as informative as
looking at a collection of interactive depth plots. Scanning across the
panels, we see each successive target depth increasing, as expected. We
also see the interactive variables increasing with increasing target
depth. Of interest here is the interaction of <code>lstat</code> with
the <code>rm</code> variable shown in the <code>rm</code> panel. Aside
from these being the strongest variables by both measures, this
interactive measure indicates the strongest connection between
variables.</p>
</div>
<div class="section level2">
<h2 id="coplots">Coplots<a class="anchor" aria-label="anchor" href="#coplots"></a>
</h2>
<p>Conditioning plots (coplots) <span class="citation">Cleveland
(1993)</span> are a powerful visualization tool to efficiently study how
a response depends on two or more variables <span class="citation">(Cleveland 1993)</span>. The method allows us to view
data by grouping observations on some conditional membership. The
simplest example involves a categorical variable, where we plot our data
conditional on class membership, for instance on the Charles river
logical variable. We can view a coplot as a stratified variable
dependence plot, indicating trends in the RF prediction results within
panels of group membership.</p>
<p>Conditional membership with a continuous variable requires
stratification at some level. Often we can make these stratification
along some feature of the variable, for instance a variable with integer
values, or 5 or 10 year age group cohorts. However in the variables of
interest in our Boston housing example, we have no “logical”
stratification indications. Therefore we will arbitrarily stratify our
variables into 6 groups of roughly equal population size using the
<code>quantile_pts</code> function. We pass the break points located by
<code>quantile_pts</code> to the <code>cut</code> function to create
grouping intervals, which we can then add to the
<code>gg_variable</code> object before plotting with the
<code>plot.gg_variable</code> function. The simple modification to
convert variable dependence plots into condition variable dependence
plots is to use the <code><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">ggplot2::facet_wrap</a></code> command to generate
a panel for each grouping interval.</p>
<p>We start by examining the predicted median home value as a function
of <code>lstat</code> conditional on membership within 6 groups of
<code>rm</code> “intervals”.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/coplots-1.png" alt="Variable Coplots. Predicted median home values as a function of percentage of lower status population, stratified by average number of rooms groups." width="672"><p class="caption">
Variable Coplots. Predicted median home values as a function of
percentage of lower status population, stratified by average number of
rooms groups.
</p>
</div>
<p>Each point in this figure is the predicted median home value response
plotted against <code>lstat</code> value conditional on <code>rm</code>
being on the interval specified. We again use the smooth loess curve to
get an idea of the trend within each group. Overall, median values
continue to decrease with increasing <code>lstat</code>, and increases
with increasing <code>rm</code>. In addition to trends, we can also
examine the conditional distribution of variables. Note that smaller
homes (<code>rm</code>) in high status (lower <code>lstat</code>)
neighborhoods still have high predicted median values, and that there
are more large homes in the higher status neighborhoods (bottom right
panel).</p>
<p>A single coplot gives us a grouped view of a variable
(<code>rm</code>), along the primary variable dimension
(<code>lstat</code>). To get a better feel for how the response depends
on both variables, it is instructive to look at the complement coplot.
We repeat the previous coplot process, predicted median home value as a
function of the <code>rm</code> variable, conditional on membership
within 6 groups <code>lstat</code> intervals.</p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/coplots2-1.png" alt="Variable Coplots. Predicted median home value as a function of average number of rooms, stratified by percentage of lower status groups." width="672"><p class="caption">
Variable Coplots. Predicted median home value as a function of average
number of rooms, stratified by percentage of lower status groups.
</p>
</div>
<p>We get similar information from this view, predicted median home
values decrease with increasing <code>lstat</code> percentage and
decreasing <code>rm</code>. However viewed together we get a better
sense of how the <code>lstat</code> and <code>rm</code> variables work
together (interact) in the median value prediction.</p>
<p>Note that typically <span class="citation">(Cleveland 1993)</span>
conditional plots for continuous variables included overlapping
intervals along the grouped variable. We chose to use mutually exclusive
continuous variable intervals for multiple reasons:</p>
<ul>
<li><p>Simplicity - We can create the coplot figures directly from the
<code>gg_variable</code> object by adding a conditional group column
directly to the object.</p></li>
<li><p>Interpretability - We find it easier to interpret and compare the
panels if each observation is only in a single panel.</p></li>
<li><p>Clarity - We prefer using more space for the data portion of the
figures than typically displayed in the <code>coplot</code> function
available in base R, which require the bar plot to present the
overlapping segments.</p></li>
</ul>
<p>It is still possible to augment the <code>gg_variable</code> to
include overlapping conditional membership with continuous variables by
duplicating rows of the object, and setting the correct conditional
group membership. The <code>plot.gg_variable</code> function recipe
above could then be used to generate the panel plot, with panels ordered
according to the factor levels of the grouping variable. We leave this
as an exercise for the reader.</p>
<div class="section level3">
<h3 id="partial-dependence-coplots">Partial dependence coplots<a class="anchor" aria-label="anchor" href="#partial-dependence-coplots"></a>
</h3>
<p>By characterizing conditional plots as stratified variable dependence
plots, the next logical step would be to generate an analogous
conditional partial dependence plot. The process is similar to variable
dependence coplots, first determine conditional group membership, then
calculate the partial dependence estimates on each subgroup using the
<code><a href="https://www.randomforestsrc.org//reference/plot.variable.rfsrc.html" class="external-link">randomForestSRC::plot.variable</a></code> function with a the
<code>subset</code> argument for each grouped interval. The
<code><a href="../reference/gg_partial_coplot.rfsrc.html">ggRandomForests::gg_partial_coplot</a></code> function is a wrapper
for generating a conditional partial dependence data object. Given a
random forest (<code><a href="https://www.randomforestsrc.org//reference/rfsrc.html" class="external-link">randomForestSRC::rfsrc</a></code> object) and a
<code>groups</code> vector for conditioning the training data set
observations, <code>gg_partial_coplot</code> calls the
<code><a href="https://www.randomforestsrc.org//reference/plot.variable.rfsrc.html" class="external-link">randomForestSRC::plot.variable</a></code> function for a set of
training set observations conditional on <code>groups</code> membership.
The function returns a <code>gg_partial_coplot</code> object, a sub
class of the <code>gg_partial</code> object, which can be plotted with
the <code>plot.gg_partial</code> function.</p>
<p>The following code block will generate the data object for creating
partial dependence coplot of the predicted median home value as a
function of <code>lstat</code> conditional on membership within the 6
groups of <code>rm</code> “intervals” that we examined in the previous
section.</p>
<pre class="text"><code>&gt; partial_coplot_boston &lt;- gg_partial_coplot(rfsrc_boston, 
+                                            xvar = "lstat", 
+                                            groups = rm_grp)</code></pre>
<p>Since the <code>gg_partial_coplot</code> makes a call to
<code><a href="https://www.randomforestsrc.org//reference/plot.variable.rfsrc.html" class="external-link">randomForestSRC::plot.variable</a></code> for each group (6) in the
conditioning set, we again resort to the data caching strategy, and load
the stored result data from the package. We modify the legend label to
indicate we’re working with groups of the “Room” variable, and use the
<code>palette="Set1"</code> from the <code>RColorBrewer</code> package
<span class="citation">(Neuwirth 2014)</span> to choose a nice color
theme for displaying the six curves.</p>
<pre class="text"><code>&gt; # Load the stored partial coplot data.
&gt; 
&gt; # # Partial coplot
&gt; ggplot(partial_coplot_boston, aes(x = lstat, y = yhat, col = group)) +
+   geom_point() +
+   geom_smooth(method = "loess", formula = "y ~ x",
+               se = FALSE, alpha = 0.25) +
+   labs(x = st_labs["lstat"], y = st_labs["medv"],
+        color = "Room", shape = "Room") +
+   scale_color_brewer(palette = "Set1")</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/prtl-coplots-1.png" alt="Partial Coplots. Risk adjusted predicted median value as a function of Lower Status, conditional on groups of average number of rooms." width="672"><p class="caption">
Partial Coplots. Risk adjusted predicted median value as a function of
Lower Status, conditional on groups of average number of rooms.
</p>
</div>
<p>Unlike variable dependence coplots, we do not need to use a panel
format for partial dependence coplots because we are looking risk
adjusted estimates (points) instead of population estimates. The figure
has a loess curve through the point estimates conditional on the
<code>rm</code> interval groupings. The figure again indicates that
larger homes (<code>rm</code> from 6.87 and up, shown in yellow) have a
higher median value then the others. In neighborhoods with higher
<code>lstat</code> percentage, the Median values decrease with
<code>rm</code> until it stabilizes from the intervals between 5.73 and
6.47, then decreases again for values smaller than 5.73. In lower
<code>lstat</code> neighborhoods, the effect of smaller <code>rm</code>
is not as noticeable.</p>
<p>We can view the partial coplot curves as slices along a surface
viewed into the page, either along increasing or decreasing
<code>rm</code> values. This is made more difficult by our choice to
select groups of similar population size, as the curves are not evenly
spaced along the <code>rm</code> variable. We return to this problem in
the next section.</p>
<p>We also construct the complement view, for partial dependence coplot
of the predicted median home value as a function of <code>rm</code>
conditional on membership within the 6 groups of <code>lstat</code>
“intervals”, and cache the following <code>gg_partial_coplot</code> data
call, and plot the results with the <code>plot.gg_variable</code>
call:</p>
<pre class="text"><code>&gt; partial_coplot_boston2 &lt;- gg_partial_coplot(rfsrc_boston, 
+                                             xvar = "rm", 
+                                             groups = lstat_grp)</code></pre>
<pre class="text"><code>&gt; # Partial coplot
&gt; #plot(partial_coplot_boston2)+ ## again plot.gg_partial_coplot
&gt; ggplot(partial_coplot_boston2, aes(x = rm, y = yhat, col = group)) +
+   geom_point() +
+   geom_smooth(method = "loess", formula = "y ~ x", se = FALSE) +
+   labs(x = st_labs["rm"], y = st_labs["medv"], 
+        color = "Lower Status", shape = "Lower Status") +
+   scale_color_brewer(palette = "Set1")</code></pre>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/prtl-coplots2-1.png" alt="Partial Coplots. Risk adjusted predicted median value as a function of average number of rooms, conditional on groups of percentage of lower status population." width="672"><p class="caption">
Partial Coplots. Risk adjusted predicted median value as a function of
average number of rooms, conditional on groups of percentage of lower
status population.
</p>
</div>
<p>This figure indicates that the median home value does not change much
until the <code>rm</code> increases above 6.5, then flattens again above
8, regardless of the <code>lstat</code> value. This agrees well with the
<code>rm</code> Partial Dependence (\autoref{partial-dependence) plots
shown earlier. Again, care must be taken in interpreting the even
spacing of these curves along the percentage of <code>lstat</code>
groupings, as again, we chose these groups to have similar sized
populations, not to be evenly spaced along the <code>lstat</code>
variable.</p>
</div>
</div>
<div class="section level2">
<h2 id="partial-plot-surfaces">Partial plot surfaces<a class="anchor" aria-label="anchor" href="#partial-plot-surfaces"></a>
</h2>
<p>Visualizing two dimensional projections of three dimensional data is
difficult, though there are tools available to make the data more
understandable. To make the interplay of lower status and average room
size a bit more understandable, we will generate a contour partial plot
of the median home values. We could generate this figure with the coplot
data we already have, but the resolution would be a bit strange. To
generate the plot of <code>lstat</code> conditional on <code>rm</code>
groupings, we would end up with contours over a grid of
<code>lstat</code>=
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">25 \times</annotation></semantics></math><code>rm</code>=
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>6</mn><annotation encoding="application/x-tex">6</annotation></semantics></math>,
for the alternative <code>rm</code> conditional on <code>lstat</code>
groups, we’d have the transpose grid of <code>lstat</code>=
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">6 \times</annotation></semantics></math><code>rm</code>=
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>25</mn><annotation encoding="application/x-tex">25</annotation></semantics></math>.</p>
<p>Since we are already using the data caching strategy, we will
generate another set of <code>gg_partial</code> objects with increased
resolution in both the <code>lstat</code> and <code>rm</code>
dimensions. For this exercise, we will find 50 points evenly spaced
along the <code>rm</code> variable values, and generate a partial plot
curve for each point. For these partial plots, we will evaluate the risk
adjusted median home value over <code>npts=50</code> points along the
<code>lstat</code> variable. This code block finds 50 <code>rm</code>
values evenly spaced along the distribution of <code>rm</code>.</p>
<pre class="text"><code>&gt; # Find the quantile points to create 50 cut points
&gt; rm_pts &lt;- quantile_pts(rfsrc_boston$xvar$rm, 
+                        groups = 49, intervals = TRUE)</code></pre>
<p>We use the following data call to generate the partial plots with the
<code><a href="https://www.randomforestsrc.org//reference/plot.variable.rfsrc.html" class="external-link">randomForestSRC::plot.variable</a></code> call. Within the lapply
call, we use scope to modify the value of the <code>rm</code> variable
within the <code>rfsrc_boston</code> training set. Since all values in
the training set are the same, the averaged value of <code>rm</code>
places each partial plot curve at a specific value of <code>rm</code>.
This code block took about 20 minutes to run on a quad core Mac Air
using a single processor.</p>
<p>The cached data is stored in the <code>partial_boston_surf</code>
data set in the package. The data set is a <code>list</code> of 50
<code>plot.variable</code> objects. This code block loads the data,
converts the <code>plot.variable</code> objects to
<code>gg_partial</code> objects, attaches numeric values for the
<code>rm</code> variable, and generates the contour plot.</p>
<pre class="text"><code>&gt; # Generate the gg_partial_coplot data object
&gt; system.time(partial_boston_surf &lt;- lapply(rm_pts, function(ct) {
+   rfsrc_boston$xvar$rm &lt;- ct
+   plot.variable(rfsrc_boston, xvar = "lstat", time = 1,
+                 npts = 50, show.plots = TRUE, 
+                 partial = TRUE)
+ }))
&gt; #     user   system  elapsed 
&gt; #    49.702   0.776  51.006 </code></pre>
<p><img src="ggrfRegression_files/figure-html/prtl-surface-1.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-2.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-3.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-4.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-5.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-6.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-7.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-8.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-9.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-10.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-11.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-12.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-13.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-14.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-15.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-16.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-17.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-18.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-19.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-20.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-21.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-22.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-23.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-24.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-25.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-26.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-27.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-28.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-29.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-30.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-31.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-32.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-33.png" width="700" style="display: block; margin: auto;"><img src="ggrfRegression_files/figure-html/prtl-surface-34.png" width="700" style="display: block; margin: auto;"></p>
<div class="figure" style="text-align: center">
<img src="ggrfRegression_files/figure-html/contour3d-1.png" alt="Partial coplot contour plot. Contours of median home value along the lstat/rm plane." width="672"><p class="caption">
Partial coplot contour plot. Contours of median home value along the
lstat/rm plane.
</p>
</div>
<p>The contours are generated over the raw <code>gg_partial</code>
estimation points, not smooth curves as shown in the Partial Dependence
(\autoref{partial-dependence) plots and Partial Coplots
(\autoref{partial-dependence-coplots) figures previously. Contour lines,
like topographic maps, are concentrated where the slope of the surface
is large. We use color to indicate the direction of the contour lines,
so that lower median home values are concentrated in the lower right
hand corner, and the values increase along the diagonal toward the upper
right. The close contour lines indicate some thing like a step in values
at 7 and 7.5 rooms, and at 5, 10 and 15% lstat.</p>
<p>Contour plots are still a little difficult to interpret. However, we
can also generate a surface with this data using the package <span class="citation">(Soetaert 2014)</span> and the
<code><a href="https://rdrr.io/pkg/plotly/man/plot_ly.html" class="external-link">plotly::plot_ly</a></code> function. Viewed in 3D, a surface can help
to better understand what the contour lines are showing us.</p>
<p>These figures reinforce the previous findings, where lower home
values are associated with higher <code>lstat</code> percentage, and
higher values are associated with larger <code>rm</code>. The difference
in this figure is we can see how the predicted values change as we move
around the map of <code>lstat</code> and <code>rm</code> combinations.
We do still need to be careful though, as partial plots average over
values on the surface that are note supported by actual
observations.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>In this vignette, we have demonstrated the use of the package to
explore a regression random forest built with the package. We have shown
how to create a random forest () model and determine which variables
contribute to the forest prediction accuracy using both VIMP () and
Minimal Depth () measures. We outlined how to investigate variable
associations with the response variable using variable dependence () and
the risk adjusted partial dependence () plots. We’ve also explored
variable interactions by using pairwise minimal depth interactions ()
and directly viewed these interactions using variable dependence coplots
() and partial dependence coplots (). Along the way, we’ve demonstrated
the use of additional commands from the package for modifying and
customizing results from .</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Becker:1988" class="csl-entry">
Becker, R. A., J. M. Chambers, and A. R. Wilks. 1988. <em>The New s
Language.</em> Wadsworth <span>&amp;</span> Brooks/Cole.
</div>
<div id="ref-Belsley:1980" class="csl-entry">
Belsley, D. A., E. Kuh, and R. E. Welsch. 1980. <em>Regression
Diagnostics. Identifying Influential Data and Sources of
Collinearity.</em> John Wiley &amp; Sons, New York.
</div>
<div id="ref-Breiman:1996" class="csl-entry">
Breiman, L. 1996a. <span>“<span>Bagging Predictors</span>.”</span>
<em>Machine Learning</em> 26: 123–40.
</div>
<div id="ref-BreimanOOB:1996e" class="csl-entry">
———. 1996b. <span>“<span>Out–Of–Bag Estimation</span>.”</span>
Statistics Department, University of California,Berkeley, CA. 94708. <a href="https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf" class="external-link">https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf</a>.
</div>
<div id="ref-Breiman:2001" class="csl-entry">
Breiman, Leo. 2001. <span>“<span>Random Forests</span>.”</span>
<em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-cart:1984" class="csl-entry">
Breiman, L, Jerome H Friedman, R Olshen, and C Stone. 1984. <em><span class="nocase">Classification and Regression Trees</span></em>.
Monterey, CA: Wadsworth; Brooks.
</div>
<div id="ref-chambers:1992" class="csl-entry">
Chambers, J. M. 1992. <em>Statistical Models in </em>. Wadsworth
<span>&amp;</span> Brooks/Cole.
</div>
<div id="ref-shiny:2015" class="csl-entry">
Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan
McPherson. 2015. <em>: Web Application Framework for </em>. <a href="https://CRAN.R-project.org/package=shiny" class="external-link">https://CRAN.R-project.org/package=shiny</a>.
</div>
<div id="ref-cleveland:1981" class="csl-entry">
Cleveland, William S. 1981. <span>“<span class="nocase">LOWESS: A
Program for Smoothing Scatterplots by Robust Locally Weighted
Regression</span>.”</span> <em>The American Statistician</em> 35 (1):
54.
</div>
<div id="ref-cleveland:1993" class="csl-entry">
———. 1993. <em>Visualizing Data</em>. Summit Press.
</div>
<div id="ref-cleveland:1988" class="csl-entry">
Cleveland, William S., and Susan J. Devlin. 1988. <span>“<span class="nocase">Locally-Weighted Regression: An Approach to Regression
Analysis by Local Fitting</span>.”</span> <em>Journal of the American
Statistical Association</em> 83 (403): 596–610.
</div>
<div id="ref-rcore" class="csl-entry">
Core Team. 2014. <em>: A Language and Environment for Statistical
Computing</em>. Vienna, Austria: Foundation for Statistical Computing.
<a href="https://www.R-project.org/" class="external-link">https://www.R-project.org/</a>.
</div>
<div id="ref-bootstrap:1994" class="csl-entry">
Efron, Bradley, and Robert Tibshirani. 1994. <em>An Introduction to the
Bootstrap</em>. <span>Chapman &amp; Hall/CRC</span>.
</div>
<div id="ref-Friedman:2000" class="csl-entry">
Friedman, Jerome H. 2000. <span>“Greedy Function Approximation: A
Gradient Boosting Machine.”</span> <em>Annals of Statistics</em> 29:
1189–1232.
</div>
<div id="ref-Harrison:1978" class="csl-entry">
Harrison, D., and D. L. Rubinfeld. 1978. <span>“Hedonic Prices and the
Demand for Clean Air.”</span> <em>J. Environ. Economics and
Management</em> 5: 81–102.
</div>
<div id="ref-StatisticalLearning:2009" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2009. <em>The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction</em>. Second. New York: Springer-Verlag.
</div>
<div id="ref-Ishwaran:2007" class="csl-entry">
Ishwaran, Hemant. 2007. <span>“<span class="nocase">Variable Importance
in Binary Regression Trees and Forests</span>.”</span> <em>Electronic
Journal of Statistics</em> 1: 519–37.
</div>
<div id="ref-Ishwaran:2007a" class="csl-entry">
Ishwaran, Hemant, and Udaya B. Kogalur. 2007. <span>“<span class="nocase">Random Survival Forests for </span>.”</span><em>
News</em> 7: 25–31.
</div>
<div id="ref-Ishwaran:RFSRC:2014" class="csl-entry">
———. 2014. <span>“<span class="nocase">Random Forests for Survival,
Regression and Classification (RF-SRC), package version
1.6.</span>”</span> <a href="https://CRAN.R-project.org/package=randomForestSRC" class="external-link">https://CRAN.R-project.org/package=randomForestSRC</a>.
</div>
<div id="ref-Ishwaran:2008" class="csl-entry">
Ishwaran, Hemant, Udaya B. Kogalur, Eugene H. Blackstone, and Michael S.
Lauer. 2008. <span>“<span>Random Survival Forests</span>.”</span>
<em>The Annals of Applied Statistics</em> 2 (3): 841–60.
</div>
<div id="ref-Ishwaran:2011" class="csl-entry">
Ishwaran, Hemant, Udaya B. Kogalur, Xi Chen, and Andy J. Minn. 2011.
<span>“Random Survival Forests for High–Dimensional Data.”</span>
<em>Statist. Anal. Data Mining</em> 4: 115–32.
</div>
<div id="ref-Ishwaran:2010" class="csl-entry">
Ishwaran, Hemant, Udaya B. Kogalur, Eiran Z. Gorodeski, Andy J. Minn,
and Michael S. Lauer. 2010. <span>“High–Dimensional Variable Selection
for Survival Data.”</span> <em>J. Amer. Statist. Assoc.</em> 105:
205–17.
</div>
<div id="ref-Liaw:2002" class="csl-entry">
Liaw, Andy, and Matthew Wiener. 2002. <span>“Classification and
Regression by .”</span><em> News</em> 2 (3): 18–22.
</div>
<div id="ref-rcolorbrewer:2014" class="csl-entry">
Neuwirth, Erich. 2014. <em>: ColorBrewer Palettes</em>. <a href="https://CRAN.R-project.org/package=RColorBrewer" class="external-link">https://CRAN.R-project.org/package=RColorBrewer</a>.
</div>
<div id="ref-plot3D:2014" class="csl-entry">
Soetaert, Karline. 2014. <em>: Plotting Multi-Dimensional Data.</em> <a href="https://CRAN.R-project.org/package=plot3D" class="external-link">https://CRAN.R-project.org/package=plot3D</a>.
</div>
<div id="ref-Tukey:1977" class="csl-entry">
Tukey, John W. 1977. <em>Exploratory Data Analysis</em>. Pearson.
</div>
<div id="ref-mass:2002" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics
with </em>. Fourth. New York: Springer-Verlag. <a href="https://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">https://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-Wickham:2009" class="csl-entry">
Wickham, Hadley. 2009. <em>: Elegant Graphics for Data Analysis</em>.
New York: Springer-Verlag.
</div>
<div id="ref-Xie:2013" class="csl-entry">
Xie, Yihui. 2013. <em>Dynamic Documents with and </em>. Boca Raton,
Florida: Chapman; Hall/CRC. <a href="https://yihui.org/knitr/" class="external-link">https://yihui.org/knitr/</a>.
</div>
<div id="ref-Xie:2014" class="csl-entry">
———. 2014. <span>“: A Comprehensive Tool for Reproducible Research in
.”</span> In <em>Implementing Reproducible Computational Research</em>,
edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng.
Chapman; Hall/CRC. <a href="https://www.crcpress.com/product/isbn/9781466561595" class="external-link">https://www.crcpress.com/product/isbn/9781466561595</a>.
</div>
<div id="ref-Xie:2015" class="csl-entry">
———. 2015. <em>: A General-Purpose Package for Dynamic Report Generation
in </em>. <a href="https://yihui.org/knitr/" class="external-link">https://yihui.org/knitr/</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by John Ehrlinger.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

      </footer>
</div>






  </body>
</html>
