---
title: 'ggRandomForests: Random Forests for Regression'
author: "John Ehrlinger"
date: "December 16, 2014"
output: html_document
---

# About this document

This a vignette for the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package, Visually Exploring Random Forests: Data and Plotting for Random Forest Models.

[ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) is designed to help uncover variable associations in the random forests models. The package is designed for use with the [randomForestSRC](http://CRAN.R-project.org/package=randomForestSRC) package for survival, regression and classification forests and [ggplot2](http://CRAN.R-project.org/package=ggplot2) package for plotting results. ggRandomForests is structured to extract intermediate data objects from the random forest object. A set of S3 functions are provided for printing and plotting each of the the ggRandomForests data objects.

This vignette is written in [markdown](http://daringfireball.net/projects/markdown/), a wiki type language for creating documents. It is support in R using the [rmarkdown](http://rmarkdown.rstudio.com) package, which is especially easy to use within the [RStudio](http://rstudio.com) IDE. A markdown/rmarkdown cheat sheet is available online at (http://rmarkdown.rstudio.com/RMarkdownCheatSheet.pdf). 

This vignette is available within the [ggRandomForests](http://cran.r-project.org/web/packages/ggRandomForests/index.html) on [CRAN](http://cran.r-project.org). The latest development version of ggRandomForests is available on the package [Github](https://github.com) repository at (https://github.com/ehrlinger/ggRandomForests).

```{r setup, include = FALSE, cache = FALSE, echo = FALSE} 
library(knitr)
# set global chunk options for knitr. These can be changed in the header for each individual R code chunk
opts_chunk$set(fig.path = 'fig-rfr/rfr-', 
               fig.align = 'center', 
               size = 'footnotesize', 
               prompt = TRUE, 
               comment = NA, 
               echo = TRUE, results = TRUE, 
               message = FALSE, warning = FALSE, 
               error = FALSE, prompt = TRUE)

# Setup the R environment
options(object.size = Inf, expressions = 100000, memory = Inf, 
        replace.assign = TRUE, width = 90)

#################
# Load_packages #
#################
library(ggplot2) # Graphics engine for generating all types of plots

library(dplyr) # Better data manipulations
library(tidyr)
library(parallel)

library(ggRandomForests)

# Analysis packages.
library(randomForestSRC) 
library(RColorBrewer)

library(xtable)

options(mc.cores = 1, rf.cores = 1)

#########################################################################
# Default computation settings
#########################################################################
theme_set(theme_bw())
```

## Introduction

Random Forests [Breiman:2001] (RF) are a fully non-parametric statistical method requiring no distributional assumptions on covariate relation to the response. RF are a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random Forests for survival [Ishwaran:2007a, Ishwaran:2008] (RF-S) are an extension of Breiman's RF techniques to survival settings, allowing efficient non-parametric analysis of time to event data. The [randomForestSRC](http://CRAN.R-project.org/package=ggRandomForests) package [Ishwaran:RFSRC:2014] is a unified treatment of Breiman's random forests for survival, regression and classification problems.

Predictive accuracy make RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package for visually exploring random forest models. The ggRandomForests package is structured to extract intermediate data objects from randomForestSRC objects and generate figures using the[ggplot2](http://CRAN.R-project.org/package=ggplot2) graphics package [Wickham:2009].

This document is formatted as a tutorial for using the randomForestSRC for building random forests and the ggRandomForests package for investigating how the forest is constructed. This tutorial uses the Boston Housing Data  available in the [MASS](http://CRAN.R-project.org/package=MASS) package. We use Variable Importance measure (VIMP) [Breiman:2001] as well as Minimal Depth [Ishwaran:2010], a property derived from the construction of each tree within the forest, to assess the impact of variables on forest prediction. We will also demonstrate the use of variable dependence plots [Friedman:2000] to aid interpretation RF results in different response settings. 

## Data: Boston Housing Data

The Bostong Housing data is a standard benchmark data set for regression models. It contains data for 506 census tracts of Boston from the 1970 census. The data is available in multiple R packages, but to keep the dependencies for the ggRandomForests package down, we will use the data contained in the [MASS](http://CRAN.R-project.org/package=MASS) package available in base install of R. The following code block loads the data into the environment. 


```{r datastep} 
data(Boston, package="MASS")
## Set modes correctly. For binary variables: transform to logical
## Check for range of 0, 1
## There is probably a better way to do this.
Boston$chas <- as.logical(Boston$chas)
```

The table details the variable names, type and descriptions contained within the Boston data set.
```{r cleanup, echo=FALSE, results="asis"}
cls <- sapply(Boston, class) 
# 
labels <- 
  #crim
  c("per capita crime rate by town.",
    # zn
    "proportion of residential land zoned for lots over 25,000 sq.ft.",
    # indus
    "proportion of non-retail business acres per town.",
    # chas
    "Charles River dummy variable (if tract bounds river).",
    # nox
    "nitrogen oxides concentration (parts per 10 million).",
    # rm
    "average number of rooms per dwelling.",
    # age
    "proportion of owner-occupied units built prior to 1940.",
    # dis
    "weighted mean of distances to Boston employment center.",
    # rad
    "index of accessibility to radial highways.",
    # tax
    "full-value property-tax rate per $10,000.",
    # ptratio
    "pupil-teacher ratio by town.",
    # black
    "Proportion of blacks by town.",
    # lstat
    "lower status of the population (percent).",
    # medv
    "median value of owner-occupied homes in $1000s.")

dta.labs <- data.frame(cbind(names = names(cls), label = labels, type = cls))

st.labs <- as.character(dta.labs$label)
names(st.labs) <- rownames(dta.labs)
print(xtable(dta.labs%>% select(-names)), type="html")
```

It is good practice to view your data before beginning an analysis. We create two figures, one panel set of categorical variables, and one panel set of continuous variables. These figures help to find outliers and missing values in each panel before getting to deep. We have made a [Shiny](shiny.rstudio.com) available at (https://ehrlinger.shinyapps.io/xportEDA) to create similar figures for arbitrary data set. 

In this case the data consists almost entirely of continuous variables with the exception of tracts along the Charles river. A simple visualization for the data is then a series of panels for each variable, with observations colored by the logical Charles river variable.  Missing values are indicated by the rug marks along the x-axis. The data is plotted along a continuous variable on the X-axis, in this case the response variable medv - the median value of homes.

```{r data}
# Use tidyr to transform the data into long format.
dta <- Boston %>% gather(variable, value, -medv, -chas)

# plot panels for each covariate colored by the logical chas variable.
ggplot(dta, aes(x=medv, y=value, color=chas))+
  geom_point()+
  geom_rug(data=dta%>% filter(is.na(value)))+
  labs(y="")+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)
```

This figure is loosely related to the pairs plot, but only examines the relation between one variable against the remainder. Plotting the data against the response variable also gives us a "sanity check" when viewing our model results. It's pretty obvious that we should find a relation between median home values and the lstat and rm variables.

## Random Forest - Regression

A Random Forest is built up by bagging [Breiman:1996] a collection of classification and regression trees [cart:1984] (CART). The method uses a set of B bootstrap [bootstrap:1994] samples, growing a set of independent tree models on each sub-sample of the population. Each trees is grown by recursively partitioning the population based on optimization of a split rule over the p-dimensional covariate space. At each split, a subset of m <= p candidate variables are chosen for the split. Each node is split into two daughter nodes by maximizing the separation of observations according the split rule. In regression trees, node impurity is measured by mean squared error, whereas in classification problems, the Gini index is used [Friedman:2000]. Each subsequent daughter node is then split until the process reaches the stopping criteria of either node purity or node member size, which defines the set of terminal (unsplit) nodes for the tree. 

Random Forests sort each training set observation into one unique terminal node per tree. The Random Forest estimate for each observation is then  calculated by aggregating, averaging (regression) or votes (classification), the terminal node results across the collection of B trees.

We grow the random forest using the rfsrc command. For this example we will use the default set of B=1000 trees (ntree argument), m=5 candidate variables (mtry) for each split with a stopping criteria of at most 5 observations within each terminal node. Because growing random forests are computationally expensive, and this vignette is really targetted at the visualization of the forest, we use a cached copy of the randomForestSRC::rfsrc object, available as a dataset in the ggRandomForests package. The actual rfsrc call is included in the comment in the following code block. 

```{r randomforest}
# Boston_rfsrc <- rfsrc(medv~., data=Boston)
data(Boston_rfsrc)
Boston_rfsrc
```

The randomForestSRC::print.rfsrc command details the forest parameters used for the call, and retun the generalization error estimate of the forest.

One advantage of Random Forests is a built in generalization error estimate. Each bootstrap sample selects approximately 63.2% of the population on average. The remaining 36.8% of observations, the Out-of-Bag~ [BreimanOOB:1996e] (OOB) sample, can be used as a hold out test set for each tree. An OOB prediction error estimate can be calculated for each observation by predicting the response over the set of trees which were NOT trained with that particular observation. Out-of-Bag prediction error estimates have been shown to be nearly identical to n--fold cross validation estimates [StatisticalLearning:2009]. This feature of Random Forests allows us to obtain both model fit and validation in one pass of the algorithm.

The `gg_error` function operates on the randomForestSRC::rfsrc object to pull out the error estimates as the forest is grown. This demonstrates the design philosophy of the ggRandomForests package, to first create a data object and provide S3 functions to operate on the data. The following code block first creates a `gg_error` object, then uses the `plot.gg_error` function to createa ggplot object, which is shown directly into the document.

```{r error}
gg_e <- gg_error(Boston_rfsrc)
plot(gg_e)
```

This figure shows that it does not take a large number of trees to stabilize the forest prediction error estimate. However, to ensure that variables have enough chance to be included in the forest prediction process, we do want to run a rather large number of trees. 

The `gg_rfsrc` function pulls out random forest the OOB prediction estimates. This call does the data extraction and plotting in one line, since we are not interested in holding the prediction estimates for later reuse. Also note that we can add in additional ggplot2 commands to modify the plot object. The S3 plot commands all return ggplot objects, which we can save for modification or resuse later in the analysis. 

```{r rfsrc}
plot(gg_rfsrc(Boston_rfsrc))+
  coord_cartesian(ylim=c(5,49))
```

The `gg_rfsrc` plot shows the predicted median home value for each observation in the training set. The estimates are OOB estimates and analogous to test set estimates. The boxplot is shown to give an indication of the distribution of these estimates. The figure is really another sanity check, as we are more interested in exploring the "why" of these predictions in this particular exercise.

## Variables


```{r vimp}
plot(gg_vimp(Boston_rfsrc))
```

```{r minimaldepth}
# Boston_var <- var.select(Boston_rfsrc)

data(Boston_var)
plot(gg_minimal_depth(Boston_var))
```

```{r minimalvimp}
plot(gg_minimal_vimp(Boston_var))
```


## Dependencies
```{r variable}
gg_v <- gg_variable(Boston_rfsrc)
xvar <- colnames(Boston)
xvar <- xvar[-which(colnames(Boston)=="chas")]
plot(gg_v, xvar=xvar,panel=TRUE, se=FALSE, span=1.2)+
  labs(y="Median Value", x="")

```

```{r chas}
plot(gg_v, xvar="chas", se=FALSE, notch=TRUE)+
  labs(y="Median Value")+
  coord_cartesian(ylim=c(5,49))
```


```{r partial}
# Boston_partial <- plot.variable(Boston_rfsrc,partial=TRUE, show.plots = FALSE )
data(Boston_partial)

gg_p <- gg_partial(Boston_partial , xvar=xvar)
ggpart <- gg_p
ggpart$chas <- NULL
plot(ggpart, xvar=xvar,panel=TRUE, se=FALSE)+
  labs(y="Median Value", x="")

```

```{r part-chas}
plot(gg_p$chas, xvar=xvar,notch=TRUE, se=FALSE, alpha=.3)+
  labs(y="Median Value")+
  coord_cartesian(ylim=c(5,49))

```

## Interactions
```{r interactions}
# Boston_int <- find.interactions(Boston_rfsrcsrc)
data(Boston_int)

plot(gg_interaction(Boston_int), xvar=Boston_var$topvars[1:6], panel=TRUE, shape=1)+
  theme(legend.position="none")
```

## Coplots

```{r coplots}
rm_grp <- cut(Boston_rfsrc$xvar$rm, breaks=6)
gg_v$rm_grp <- paste("rm=",rm_grp, sep="")

var_dep <- plot(gg_v, xvar = "lstat", smooth = TRUE, 
                method = "loess", span=1.5, alpha = .5, se = FALSE) + 
  labs(y = "Median Value") + 
  theme(legend.position = "none") + 
  scale_color_brewer(palette = "Set3") + 
  #scale_shape_manual(values = event.marks, labels = event.labels)+ 
  facet_wrap(~rm_grp)

var_dep
```

```{r prtl-copl, eval=FALSE}
Boston_prtl_coplot <- gg_partial_coplot(Boston_rfsrc, xvar="lstat", 
                                        groups=rm_grp,
                                        show.plots=FALSE)
```

```{r prtl-coplots}
data(Boston_prtl_coplot)
ggpl <- ggplot(Boston_prtl_coplot, aes(x=lstat, y=yhat, 
                                       shape=groups, 
                                       color=groups))+
  geom_point()+geom_smooth(se=FALSE)+
  labs(x=st.labs["lstat"], y="Median Value", 
       color="rm", shape="rm")+
  scale_color_brewer(palette="Set1")
ggpl
```

## Conclusion

```{r surf, echo=FALSE, eval=FALSE}
cut.vector <- function(obj, npts){
  n.x <- length(unique(obj))
  if (n.x > npts) {
    x.uniq <- sort(unique(obj))[unique(as.integer(seq(1, n.x, length = min(npts, n.x))))]
  }
  x.uniq
}
cut.vector(Boston_rfsrc$xvar$rm,npts=50)

```



# References

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.

